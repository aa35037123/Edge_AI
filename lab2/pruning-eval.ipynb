{"cells":[{"cell_type":"markdown","metadata":{"id":"RdC9V9xB10Gt"},"source":["# **Template for Torch-Pruning**\n","\n","This template is just built for your convinience.\n","\n","You are not required to follow the steps and method given below."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T09:19:10.718584Z","iopub.status.busy":"2024-03-29T09:19:10.718213Z"},"id":"7jfw5rNimfwu","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch_pruning in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (1.3.7)\n","Requirement already satisfied: torch in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch_pruning) (2.2.1)\n","Requirement already satisfied: numpy in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch_pruning) (1.26.4)\n","Requirement already satisfied: filelock in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch->torch_pruning) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch->torch_pruning) (4.9.0)\n","Requirement already satisfied: sympy in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch->torch_pruning) (1.12)\n","Requirement already satisfied: networkx in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch->torch_pruning) (3.1)\n","Requirement already satisfied: jinja2 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch->torch_pruning) (3.1.3)\n","Requirement already satisfied: fsspec in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch->torch_pruning) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from jinja2->torch->torch_pruning) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from sympy->torch->torch_pruning) (1.3.0)\n","Requirement already satisfied: torchprofile in /home/aa35037123/.local/lib/python3.10/site-packages (0.0.4)\n","Requirement already satisfied: numpy>=1.14 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torchprofile) (1.26.4)\n","Requirement already satisfied: torch>=1.4 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torchprofile) (2.2.1)\n","Requirement already satisfied: torchvision>=0.4 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torchprofile) (0.17.1)\n","Requirement already satisfied: filelock in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (4.9.0)\n","Requirement already satisfied: sympy in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (1.12)\n","Requirement already satisfied: networkx in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.1)\n","Requirement already satisfied: jinja2 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (3.1.3)\n","Requirement already satisfied: fsspec in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torch>=1.4->torchprofile) (2024.3.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from torchvision>=0.4->torchprofile) (10.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from jinja2->torch>=1.4->torchprofile) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\n"]}],"source":["!pip install --upgrade torch_pruning\n","!pip install torchprofile\n","# !pip install torch torchvision torchaudio"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"-LiF8Vrqm2FU","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/aa35037123/miniconda3/envs/lab2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torchvision\n","from torchvision.models import mobilenet_v2\n","import torch_pruning as tp\n","from functools import partial\n","import copy\n","import math\n","import random\n","import time\n","from collections import OrderedDict, defaultdict\n","from typing import Union, List\n","\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from torch import nn\n","from torch.optim import *\n","from torch.optim.lr_scheduler import *\n","from torch.utils.data import DataLoader\n","from torchprofile import profile_macs\n","from torchvision.datasets import *\n","from torchvision.transforms import *\n","from tqdm.auto import tqdm\n","import torch.nn.functional as F\n","from torchprofile import profile_macs\n","import os"]},{"cell_type":"markdown","metadata":{"id":"dQ411pYQhkw4"},"source":["## A Minimal Example   \n","In this section, you will perform channel pruning using the library [Torch-Pruning](https://github.com/VainF/Torch-Pruning).  \n","\n","The puuner in Torch-Pruning has three main functions: sparse training (optional), importance estimation, and parameter removal.  \n","Torch-pruning offers two core features to support this process:\n","\n","tp.importance(): This criteria is utilized to measure the importance of weights.  \n","\n","tp.pruner(): This is a pruner used for the actual pruning of the parameters.  \n","\n","For detailed information on this process, please refer to this [tutorial](https://github.com/VainF/Torch-Pruning/wiki/4.-High%E2%80%90level-Pruners/). Additionally, a more practical example is available in [here](https://github.com/VainF/Torch-Pruning/blob/master/benchmarks/main.py)."]},{"cell_type":"markdown","metadata":{"id":"s9e8CpBhmph9"},"source":["### 1. Load model\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"umaWm4eYms6G","trusted":true},"outputs":[],"source":["model = torch.load('./mobilenetv2_0.963.pth', map_location=\"cpu\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"mSe6NDqqm_qN"},"source":["### 2. Prepare a pruner\n","By default, Torch-Pruning will automatically prune the last non-singleton dim of these parameters. If you want to customize this behaviour, please provide an `unwrapped_parameters` list as the following example."]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["transforms = {\n","    \"train\": Compose([\n","      Resize((224, 224)),\n","      ToTensor(),\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","    \"test\": Compose([\n","      Resize((224, 224)),\n","      ToTensor(),\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","dataset = {}\n","for split in [\"train\", \"test\"]:\n","  dataset[split] = CIFAR10(\n","    root=\"data/cifar10\",\n","    train=(split == \"train\"),\n","    download=True,\n","    transform=transforms[split],\n","  )\n","\n","# You can apply your own batch_size\n","dataloader = {}\n","for split in ['train', 'test']:\n","  dataloader[split] = DataLoader(\n","    dataset[split],\n","    batch_size=10,\n","    shuffle=(split == 'train'),\n","    num_workers=0,\n","    pin_memory=True,\n","    drop_last=True\n","  )"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["def progressive_pruning(pruner, model, speed_up, example_inputs, train_loader=None):\n","    model.eval()\n","    base_ops, _ = tp.utils.count_ops_and_params(model, example_inputs=example_inputs)\n","    current_speed_up = 1\n","    while current_speed_up < speed_up:\n","#         model.zero_grad()\n","#         imp=pruner.importance\n","#         imp._prepare_model(model, pruner)\n","#         for k, (imgs, lbls) in enumerate(train_loader):\n","#             if k>=10: break\n","#             imgs = imgs.cuda()\n","#             lbls = lbls.cuda()\n","#             output = model(imgs)\n","# #             sampled_y = torch.multinomial(torch.nn.functional.softmax(output.cpu().data, dim=1),\n","# #                                               1).squeeze().cuda()\n","# #             loss_sample = F.cross_entropy(output, sampled_y)\n","#             loss_sample = nn.CrossEntropyLoss()(output, lbls)\n","#             loss_sample.backward()\n","#             imp.step()\n","        pruner.step()\n","        pruned_ops, _ = tp.utils.count_ops_and_params(model, example_inputs=example_inputs)\n","        current_speed_up = float(base_ops) / pruned_ops\n","        if pruner.current_step == pruner.iterative_steps:\n","            break\n","    return current_speed_up"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["output_dir = \"./pruning_output\"\n","def eval(model, test_loader, device=None, verbose=True):\n","\n","    num_samples = 0\n","    num_correct = 0\n","    \n","    model.to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        for inputs, targets in tqdm(test_loader, desc=\"eval\", leave=False,\n","                                    disable=not verbose):\n","            # Move the data from CPU to GPU\n","            inputs = inputs.cuda()\n","            targets = targets.cuda()\n","\n","            # Inference\n","            outputs = model(inputs)\n","\n","            # Convert logits to class indices\n","            outputs = outputs.argmax(dim=1)\n","\n","            # Update metrics\n","            num_samples += targets.size(0)\n","            num_correct += (outputs == targets).sum()\n","\n","    return (num_correct / num_samples * 100).item()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["###### choose 1\n","MFLOPs: 61.325886\n"]},{"name":"stderr","output_type":"stream","text":["                                                         "]},{"name":"stdout","output_type":"stream","text":["Acc: 0.915199966430664\n","Your Score: 38.10/45\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["model = torch.load('./pruning_output/mobilenet_0.915_61.33.pth', map_location=\"cpu\")\n","model = model.to(device)\n","example_inputs = torch.zeros(1, 3, 224, 224).to(device)\n","ops, size = tp.utils.count_ops_and_params(model, example_inputs=example_inputs)\n","print(\"###### choose 1\")\n","MFLOPs = ops/1e6\n","print(f'MFLOPs: {MFLOPs}')\n","\n","Accuracy = eval(model, dataloader['test'], device=device) / 100\n","print(f\"Acc: {Accuracy}\")\n","\n","score = max(0, (200-MFLOPs)/(200-45) - 10*min(max(0.92-Accuracy, 0), 1)) * 45\n","print(\"Your Score: {:0.2f}/45\".format(score))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["##### choose 2\n","MFLOPs: 44.622582\n"]},{"name":"stderr","output_type":"stream","text":["                                                         "]},{"name":"stdout","output_type":"stream","text":["Acc: 0.9062000274658203\n","Your Score: 38.90/45\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["model = torch.load('./pruning_output/mobilenet_0.906_44.6.pth', map_location=\"cpu\")\n","model = model.to(device)\n","example_inputs = torch.zeros(1, 3, 224, 224).to(device)\n","ops, size = tp.utils.count_ops_and_params(model, example_inputs=example_inputs)\n","print(\"##### choose 2\")\n","MFLOPs = ops/1e6\n","print(f'MFLOPs: {MFLOPs}')\n","\n","Accuracy = eval(model, dataloader['test'], device=device) / 100\n","print(f\"Acc: {Accuracy}\")\n","\n","score = max(0, (200-MFLOPs)/(200-45) - 10*min(max(0.92-Accuracy, 0), 1)) * 45\n","print(\"Your Score: {:0.2f}/45\".format(score))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["##### choose 2\n","MFLOPs: 44.622582\n"]},{"name":"stderr","output_type":"stream","text":["                                                         "]},{"name":"stdout","output_type":"stream","text":["Acc: 0.9080999755859375\n","Your Score: 39.75/45\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["model = torch.load('./pruning_output/mobilenet_0.908_44.6.pth', map_location=\"cpu\")\n","model = model.to(device)\n","example_inputs = torch.zeros(1, 3, 224, 224).to(device)\n","ops, size = tp.utils.count_ops_and_params(model, example_inputs=example_inputs)\n","print(\"##### choose 2\")\n","MFLOPs = ops/1e6\n","print(f'MFLOPs: {MFLOPs}')\n","\n","Accuracy = eval(model, dataloader['test'], device=device) / 100\n","print(f\"Acc: {Accuracy}\")\n","\n","score = max(0, (200-MFLOPs)/(200-45) - 10*min(max(0.92-Accuracy, 0), 1)) * 45\n","print(\"Your Score: {:0.2f}/45\".format(score))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"isSourceIdPinned":true,"modelInstanceId":17665,"sourceId":21333,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
