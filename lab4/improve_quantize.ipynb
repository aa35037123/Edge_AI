{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rFS5-s47wa"
      },
      "source": [
        "# Lab4: Quantize DeiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XdmT91OIoXZ"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zM6gf9u-99jp"
      },
      "outputs": [],
      "source": [
        "# install the newest version of torch, torchvision, and timm\n",
        "#!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata timm\n",
        "#!pip3 install torch torchaudio torchvision torchtext torchdata timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4OYDr727IoXb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from timm.data import create_transform\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.export import export, ExportedProgram\n",
        "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
        "from torch._export import capture_pre_autograd_graph\n",
        "from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e, prepare_qat_pt2e\n",
        "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
        "    get_symmetric_quantization_config,\n",
        "    XNNPACKQuantizer,\n",
        ")\n",
        "\n",
        "torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "M8GuVUhpfFGs"
      },
      "outputs": [],
      "source": [
        "def data_loader_to_list(data_loader, length=128):\n",
        "    new_data_loader = []\n",
        "    if length < 0:\n",
        "        return list(data_loader)\n",
        "    else:\n",
        "        for i, data in enumerate(data_loader):\n",
        "            if i >= length:\n",
        "                break\n",
        "            new_data_loader.append(data)\n",
        "\n",
        "    return new_data_loader\n",
        "\n",
        "def build_dataset_CIFAR100(is_train, data_path):\n",
        "    transform = build_transform(is_train)\n",
        "    dataset = datasets.CIFAR100(data_path, train=is_train, transform=transform, download=True)\n",
        "    nb_classes = 100\n",
        "    return dataset, nb_classes\n",
        "\n",
        "def build_transform(is_train):\n",
        "    input_size = 224\n",
        "    eval_crop_ratio = 1.0\n",
        "\n",
        "    resize_im = input_size > 32\n",
        "    if is_train:\n",
        "        # this should always dispatch to transforms_imagenet_train\n",
        "        transform = create_transform(\n",
        "            input_size=input_size,\n",
        "            is_training=True,\n",
        "            color_jitter=0.3,\n",
        "            auto_augment='rand-m9-mstd0.5-inc1',\n",
        "            interpolation='bicubic',\n",
        "            re_prob=0.0,\n",
        "            re_mode='pixel',\n",
        "            re_count=1,\n",
        "        )\n",
        "        if not resize_im:\n",
        "            # replace RandomResizedCropAndInterpolation with\n",
        "            # RandomCrop\n",
        "            transform.transforms[0] = transforms.RandomCrop(\n",
        "                input_size, padding=4)\n",
        "        return transform\n",
        "\n",
        "    t = []\n",
        "    if resize_im:\n",
        "        size = int(input_size / eval_crop_ratio)\n",
        "        t.append(\n",
        "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
        "        )\n",
        "        t.append(transforms.CenterCrop(input_size))\n",
        "\n",
        "    t.append(transforms.ToTensor())\n",
        "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
        "    return transforms.Compose(t)\n",
        "\n",
        "def prepare_data(batch_size):\n",
        "    train_set, nb_classes = build_dataset_CIFAR100(is_train=True, data_path='./data')\n",
        "    test_set, _ = build_dataset_CIFAR100(is_train=False, data_path='./data')\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    return train_loader, test_loader, nb_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate(model: nn.Module, data_loader) -> None:\n",
        "    calibration_data = data_loader_to_list(data_loader, math.ceil(128/data_loader.batch_size)) # calibrate 128 images\n",
        "    for image, _ in calibration_data:\n",
        "        model(image)\n",
        "    return\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    cnt = 0\n",
        "    for image, target in tqdm(data_loader):\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    model_size = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return model_size\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "def getMiniTestDataset():\n",
        "    # Create a test_loader with batch size = 1\n",
        "    _, test_loader, _ = prepare_data(batch_size=1)\n",
        "\n",
        "    # Prepare to collect 10 images per class\n",
        "    class_images = [[] for _ in range(100)]\n",
        "\n",
        "    # Iterate through the data\n",
        "    for (image, label) in test_loader:\n",
        "        if len(class_images[label]) < 5:\n",
        "            class_images[label].append((image, label))\n",
        "        if all(len(images) == 5 for images in class_images):\n",
        "            break  # Stop once we have 10 images per class\n",
        "\n",
        "    # flatten class_images\n",
        "    mini_test_dataset = []\n",
        "    for images in class_images:\n",
        "        mini_test_dataset.extend(images)\n",
        "    return mini_test_dataset\n",
        "\n",
        "# TA Uses the following code to evaluate your score\n",
        "def lab4_cifar100_evaluation(quantized_model_path='deits_quantized.pth'):\n",
        "    # Prepare data\n",
        "    mini_test_dataset = getMiniTestDataset()\n",
        "\n",
        "    # Load quantized model\n",
        "    quantized_ep = torch.export.load(quantized_model_path)\n",
        "    quantized_model = quantized_ep.module()\n",
        "\n",
        "    # Evaluate model\n",
        "    start_time = time.time()\n",
        "    acc = evaluate_model(quantized_model, mini_test_dataset, device=\"cpu\")\n",
        "    exec_time = time.time() - start_time\n",
        "    model_size = get_size_of_model(quantized_model)\n",
        "\n",
        "    print(f\"Model Size: {model_size:.2f} MB\")\n",
        "    print(f\"Accuracy: {acc:.2f}%\")\n",
        "    print(f\"Execution Time: {exec_time:.2f} s\")\n",
        "\n",
        "    score = 0\n",
        "    if model_size <= 30: score += 10\n",
        "    if model_size <= 27: score += 2 * math.floor(27-model_size)\n",
        "    if acc >= 86:\n",
        "      score += 10 + 2 * math.floor(acc-86)\n",
        "    print(f'Model Score: {score:.2f}')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkZ8DKOfIoXd"
      },
      "source": [
        "## Part1: Simple Quantization Pipeline (0%)\n",
        "\n",
        "Below is a naive pipeline of quantizing DeiT-S. You may need to modify the pipeline or build your own later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbo8e-DgIoXd"
      },
      "source": [
        "[**use_reference_representation=False** in **convert_pt2e()** represents fake quant (matmul using fp32).](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq_static.html#convert-the-calibrated-model-to-a-quantized-model)\n",
        "\n",
        "However when the variable is set to True, the execution speed becomes extremely slow.\n",
        "\n",
        "In this lab, it is just fine to set **use_reference_representation=False**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace functions XNNPackQuantizer uses for annotation.\n",
        "\n",
        "def get_module_names(name):\n",
        "    names = name.split(\".\")\n",
        "    return [\".\".join(names[i:]) for i in reversed(range(len(names)))]\n",
        "\n",
        "def parse_string(name):\n",
        "    if name.startswith(\"L\"): return name[10:]\n",
        "    split_getattr = name.split(\")\")\n",
        "    ig_left = split_getattr[0].split(\"L['self'].\")[-1].split(\",\")[0]\n",
        "    ig_right = split_getattr[0].split(\", '\")[-1][:-1]\n",
        "    return ig_left + \".\" + str(ig_right) + split_getattr[1]\n",
        "\n",
        "def is_name_in_ignore_list(name, IGNORE_LIST):\n",
        "    return name in IGNORE_LIST\n",
        "\n",
        "def name_not_in_ignore_list(n, IGNORE_LIST) -> bool:\n",
        "    nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "    names = [n for n, klass in nn_module_stack.values()]\n",
        "    if len(names) == 0:\n",
        "        return True\n",
        "\n",
        "    names = get_module_names(parse_string(names[-1]))\n",
        "    set1 = set(names)\n",
        "    set2 = set(IGNORE_LIST)\n",
        "    # if len(set1.intersection(set2)) == 0:\n",
        "    #     print(\"DEBUG: \", names)\n",
        "    return len(set1.intersection(set2)) == 0\n",
        "\n",
        "def get_module_name_filter(module_name: str, IGNORE_LIST):\n",
        "    def module_name_filter(n) -> bool:\n",
        "        nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "        names = [n for n, klass in nn_module_stack.values()]\n",
        "        if len(names) == 0:\n",
        "            return False\n",
        "\n",
        "        names = get_module_names(parse_string(names[-1]))\n",
        "        return (module_name in names) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "    return module_name_filter\n",
        "\n",
        "\n",
        "def get_module_type_filter(tp, IGNORE_LIST):\n",
        "    def module_type_filter(n) -> bool:\n",
        "        nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "        types = [t for _, t in nn_module_stack.values()]\n",
        "        return (tp in types) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "\n",
        "    return module_type_filter\n",
        "\n",
        "\n",
        "def get_not_module_type_or_name_filter(\n",
        "    tp_list, module_name_list, IGNORE_LIST\n",
        "):\n",
        "    module_type_filters = [get_module_type_filter(tp) for tp in tp_list]\n",
        "    module_name_list_filters = [get_module_name_filter(m) for m in module_name_list]\n",
        "\n",
        "    def not_module_type_or_name_filter(n) -> bool:\n",
        "        return not any(f(n) for f in module_type_filters + module_name_list_filters) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "\n",
        "    return not_module_type_or_name_filter\n",
        "\n",
        "class PartialXNNPACKQuantizer(XNNPACKQuantizer): # skips quantizing layers inside the ignore_list\n",
        "    def __init__(self, ignore_list):\n",
        "        super().__init__()\n",
        "        self.ignore_list = ignore_list\n",
        "\n",
        "    def _annotate_for_static_quantization_config(\n",
        "        self, model: torch.fx.GraphModule\n",
        "    ) -> torch.fx.GraphModule:\n",
        "        print(\"annotating for static quantization\")\n",
        "        module_name_list = list(self.module_name_config.keys())\n",
        "        for module_name, config in self.module_name_config.items():\n",
        "            self._annotate_all_static_patterns(\n",
        "                model, config, get_module_name_filter(module_name, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        tp_list = list(self.module_type_config.keys())\n",
        "        for module_type, config in self.module_type_config.items():\n",
        "            self._annotate_all_static_patterns(\n",
        "                model, config, get_module_type_filter(module_type, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        self._annotate_all_static_patterns(\n",
        "            model,\n",
        "            self.global_config,\n",
        "            get_not_module_type_or_name_filter(tp_list, module_name_list, self.ignore_list),\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def _annotate_for_dynamic_quantization_config(\n",
        "        self, model: torch.fx.GraphModule\n",
        "    ) -> torch.fx.GraphModule:\n",
        "        print(\"annotating for dynamic quantization\")\n",
        "        module_name_list = list(self.module_name_config.keys())\n",
        "        for module_name, config in self.module_name_config.items():\n",
        "            self._annotate_all_dynamic_patterns(\n",
        "                model, config, get_module_name_filter(module_name, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        tp_list = list(self.module_type_config.keys())\n",
        "        for module_type, config in self.module_type_config.items():\n",
        "            self._annotate_all_dynamic_patterns(\n",
        "                model, config, get_module_type_filter(module_type, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        self._annotate_all_dynamic_patterns(\n",
        "            model,\n",
        "            self.global_config,\n",
        "            get_not_module_type_or_name_filter(tp_list, module_name_list, self.ignore_list),\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# quantizer = XNNPACKQuantizer()\n",
        "# quantizer = PartialXNNPACKQuantizer(ignore_list=[\"head\"]) # replace XNNPACKQuantizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9CfbAo1PIoXe"
      },
      "outputs": [],
      "source": [
        "from torch.quantization import get_default_qconfig, prepare, convert\n",
        "\n",
        "def partial_quantize_ptq_model(quantizer, model: nn.Module, device, data_loader, per_channel=False) -> None:\n",
        "    _dummy_input_data = (next(iter(data_loader))[0].to(device),)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    model = capture_pre_autograd_graph(model, _dummy_input_data)\n",
        "\n",
        "    quantization_config = get_symmetric_quantization_config(is_per_channel=per_channel, is_qat=False)\n",
        "    quantizer.set_global(quantization_config)\n",
        "    # prepare_pt2e folds BatchNorm operators into preceding Conv2d operators, and inserts observers in appropriate places in the model.\n",
        "    model = prepare_pt2e(model, quantizer)\n",
        "\n",
        "    # model(*_dummy_input_data)\n",
        "    #get 128 input data for calibration\n",
        "    for i, (image, _) in enumerate(data_loader):\n",
        "        if i >= 128:\n",
        "            break\n",
        "        image = image.to(device)\n",
        "        model(image)\n",
        "\n",
        "    model = convert_pt2e(model, use_reference_representation=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ignore_list = [\"blocks.9.mlp.fc1\", \"blocks.2.attn.qkv\", \"blocks.10.mlp.fc1\"]\n",
        "ignore_list = [\"cls.token\", \"blocks.2.attn.qkv\"]\n",
        "quantizer = PartialXNNPACKQuantizer(ignore_list=ignore_list) # replace XNNPACKQuantizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "I_13WgBMIoXe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Before quantization:\n",
            "Device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:04<00:00, 112.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 93.6%\n",
            "Size (MB) before quantization: 86.905654\n",
            "Accuracy of the model on the test images: 93.6%\n",
            "Quantizing model...\n",
            "annotating for static quantization\n",
            "Exporting model...\n",
            "Evaluating model...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:40<00:00, 12.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 85.6%\n",
            "Model Size: 23.26 MB\n",
            "Accuracy: 85.60%\n",
            "Execution Time: 40.55 s\n",
            "Model Score: 16.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 1\n",
        "model = torch.load('0.9099_deit3_small_patch16_224.pth', map_location='cpu')\n",
        "model = model.to(device)\n",
        "train_loader, test_loader, nb_classes = prepare_data(batch_size)\n",
        "\n",
        "simple_test_loader = data_loader_to_list(test_loader)\n",
        "\n",
        "mini_test_dataset = getMiniTestDataset()\n",
        "# evaluate before quantization\n",
        "print('Before quantization:')\n",
        "print('Device:', device)\n",
        "acc = evaluate_model(model, mini_test_dataset, device)\n",
        "# acc = evaluate_model(model, test_loader, device) # acc: 90.99%\n",
        "print('Size (MB) before quantization:', get_size_of_model(model))\n",
        "print(f'Accuracy of the model on the test images: {acc}%') # 92.8%\n",
        "# evaluate_model(model, test_loader, device) # acc: 90.99%\n",
        "\n",
        "# quantize model\n",
        "print('Quantizing model...')\n",
        "model.cpu()\n",
        "quantized_model = partial_quantize_ptq_model(quantizer, model, device, train_loader, per_channel=False)\n",
        "torch.ao.quantization.move_exported_model_to_eval(quantized_model)\n",
        "\n",
        "print('Exporting model...')\n",
        "quantized_model_path = \"deits_quantized_1.pth\"\n",
        "\n",
        "quantized_model.cpu()\n",
        "cpu_example_inputs = (torch.randn([1, 3, 224, 224]), ) # batch_size should equal to 1 on inference.\n",
        "quantized_ep = torch.export.export(quantized_model, cpu_example_inputs)\n",
        "torch.export.save(quantized_ep, quantized_model_path)\n",
        "\n",
        "print('Evaluating model...')\n",
        "lab4_cifar100_evaluation(quantized_model_path) # 84.4%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Ou1NYqlNIoXf",
        "V_xk1sSXoqsJ",
        "NO1-XuLJBkL2",
        "bztBdOxJRKTD"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
