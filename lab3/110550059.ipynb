{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmi4V4VaYR6i"
      },
      "source": [
        "# **Lab 3: Quantization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tFjnZZVlIFL"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install the newest version of torch, torchvision, and timm\n",
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata timm\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zDWoVhv_wGmA",
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shigon/miniconda3/envs/executorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLcJUofTDKud",
        "metadata": {},
        "outputId": "7402108d-3416-4f11-b3ff-12de7fbc3b7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb877f94130>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGhomDjsaDB5"
      },
      "source": [
        "Test Functions **(DO NOT MODIFY!!)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOBobYuTLksn"
      },
      "source": [
        "# Part2: Quantize MobileNetV2 and Export\n",
        "\n",
        "Below shows the steps of how to quantize & convert the model.  For more details, refer to [Quantization-Aware Training](https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html)and[Post Training Quantization](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html). You may have to run it using your own machine.\n",
        "\n",
        "***The code blocks below doesen't have to be executed when you are submitting this file.***\n",
        "\n",
        " $$\n",
        "        Score = (10 \\times Step  function(Accuracy-0.88)+ 20 \\times \\dfrac{Accuracy - 0.88}{0.96 - 0.88})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u17OMq-NA5z"
      },
      "source": [
        "1. Load **mobilenet_v2** with 96.3% accuracy on CIFAR10. (Link of the model is written in the spec of lab3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nrzf8gJecuNq",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from torch._export import capture_pre_autograd_graph\n",
        "\n",
        "from torch.ao.quantization.quantize_pt2e import (\n",
        "  prepare_pt2e,\n",
        "  convert_pt2e,\n",
        "  prepare_qat_pt2e\n",
        ")\n",
        "\n",
        "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
        "  XNNPACKQuantizer,\n",
        "  get_symmetric_quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jObz2R1oMHEK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import mobilenet_v2\n",
        "# from torchvision.models.quantization import mobilenet_v2\n",
        "\n",
        "model = torch.load('./mobilenetv2_0.963.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJT66aMlPzdR"
      },
      "source": [
        "2. Quantize the model using XNNPACKQuantizer, you can choose either Post Training Quantization or Quantization-Aware Training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T8Ji9YRUOnOw",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "def prepare_data(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize images to match MobileNet input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    train_set = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_set = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,drop_last=True)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yeaohavoaQRa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def evaluate_model(model, data_loader,device):\n",
        "\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "\n",
        "    cnt = 0\n",
        "\n",
        "    for image, target in data_loader:\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_E0eTXZNd-cO",
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "device: cuda:0\n",
            "Size (MB): 9.169412\n",
            "Accuracy of the model on the test images: 96.30408653846153%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "96.30408653846153"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 128\n",
        "train_loader, test_loader = prepare_data(batch_size)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "model.eval()\n",
        "print_size_of_model(model)\n",
        "evaluate_model(model, test_loader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "r_GyxW8BNm6T",
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 95.22235576923077%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "95.22235576923077"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def quantize_ptq_model(model: nn.Module, device=\"cuda:0\") -> None:\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "\n",
        "    # Step 1. program capture\n",
        "    model.to(device)\n",
        "    example_input = (torch.randn(1, 3, 224, 224).to(device), )\n",
        "    ptq_model = capture_pre_autograd_graph(model, example_input)\n",
        "\n",
        "    # Step 2. set quantizatizer\n",
        "    # prepare_pt2e folds BatchNorm operators into preceding Conv2d operators, and inserts observers in appropriate places in the model.\n",
        "    quantizer = XNNPACKQuantizer()\n",
        "    quantizer.set_global(get_symmetric_quantization_config())\n",
        "\n",
        "    # Step 3. prepare pt2e\n",
        "    ptq_model = prepare_pt2e(ptq_model, quantizer)\n",
        "    \n",
        "    # calibration\n",
        "    def calibrate(model, data_loader):\n",
        "        with torch.no_grad():\n",
        "            for images, _ in data_loader:\n",
        "                images = images.to(device)\n",
        "                model(images)\n",
        "        return\n",
        "    calibrate(ptq_model, train_loader)\n",
        "\n",
        "    # Step 4. convert model\n",
        "    ptq_model = convert_pt2e(ptq_model)\n",
        "\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "    return ptq_model\n",
        "\n",
        "\n",
        "ptq_model = quantize_ptq_model(model, device=device)\n",
        "torch.ao.quantization.move_exported_model_to_eval(ptq_model)\n",
        "print_size_of_model(ptq_model)\n",
        "evaluate_model(ptq_model, test_loader,device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save PTQ model\n",
        "file_path = './mobilenetv2_ptq.pth'\n",
        "example_inputs = (next(iter(test_loader))[0].to(device),)\n",
        "quantized_ep = torch.export.export(ptq_model, example_inputs)\n",
        "torch.export.save(quantized_ep, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 95.22235576923077%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "95.22235576923077"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load PTQ model\n",
        "loaded_quantized_ep = torch.export.load('./mobilenetv2_ptq.pth')\n",
        "loaded_quantized_model = loaded_quantized_ep.module()\n",
        "\n",
        "evaluate_model(loaded_quantized_model, test_loader,device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2tFjnZZVlIFL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
