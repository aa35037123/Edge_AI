{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rFS5-s47wa"
      },
      "source": [
        "# Lab4: Quantize DeiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XdmT91OIoXZ"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zM6gf9u-99jp"
      },
      "outputs": [],
      "source": [
        "# install the newest version of torch, torchvision, and timm\n",
        "#!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata timm\n",
        "#!pip3 install torch torchaudio torchvision torchtext torchdata timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4OYDr727IoXb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from timm.data import create_transform\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.export import export, ExportedProgram\n",
        "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
        "from torch._export import capture_pre_autograd_graph\n",
        "from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e, prepare_qat_pt2e\n",
        "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
        "    get_symmetric_quantization_config,\n",
        "    XNNPACKQuantizer,\n",
        ")\n",
        "import copy\n",
        "\n",
        "torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M8GuVUhpfFGs"
      },
      "outputs": [],
      "source": [
        "def data_loader_to_list(data_loader, length=128):\n",
        "    new_data_loader = []\n",
        "    if length < 0:\n",
        "        return list(data_loader)\n",
        "    else:\n",
        "        for i, data in enumerate(data_loader):\n",
        "            if i >= length:\n",
        "                break\n",
        "            new_data_loader.append(data)\n",
        "\n",
        "    return new_data_loader\n",
        "\n",
        "def build_dataset_CIFAR100(is_train, data_path):\n",
        "    transform = build_transform(is_train)\n",
        "    dataset = datasets.CIFAR100(data_path, train=is_train, transform=transform, download=True)\n",
        "    nb_classes = 100\n",
        "    return dataset, nb_classes\n",
        "\n",
        "def build_transform(is_train):\n",
        "    input_size = 224\n",
        "    eval_crop_ratio = 1.0\n",
        "\n",
        "    resize_im = input_size > 32\n",
        "    if is_train:\n",
        "        # this should always dispatch to transforms_imagenet_train\n",
        "        transform = create_transform(\n",
        "            input_size=input_size,\n",
        "            is_training=True,\n",
        "            color_jitter=0.3,\n",
        "            auto_augment='rand-m9-mstd0.5-inc1',\n",
        "            interpolation='bicubic',\n",
        "            re_prob=0.0,\n",
        "            re_mode='pixel',\n",
        "            re_count=1,\n",
        "        )\n",
        "        if not resize_im:\n",
        "            # replace RandomResizedCropAndInterpolation with\n",
        "            # RandomCrop\n",
        "            transform.transforms[0] = transforms.RandomCrop(\n",
        "                input_size, padding=4)\n",
        "        return transform\n",
        "\n",
        "    t = []\n",
        "    if resize_im:\n",
        "        size = int(input_size / eval_crop_ratio)\n",
        "        t.append(\n",
        "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
        "        )\n",
        "        t.append(transforms.CenterCrop(input_size))\n",
        "\n",
        "    t.append(transforms.ToTensor())\n",
        "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
        "    return transforms.Compose(t)\n",
        "\n",
        "def prepare_data(batch_size):\n",
        "    train_set, nb_classes = build_dataset_CIFAR100(is_train=True, data_path='./data')\n",
        "    test_set, _ = build_dataset_CIFAR100(is_train=False, data_path='./data')\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    return train_loader, test_loader, nb_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate(model: nn.Module, data_loader) -> None:\n",
        "    calibration_data = data_loader_to_list(data_loader, math.ceil(128/data_loader.batch_size)) # calibrate 128 images\n",
        "    for image, _ in calibration_data:\n",
        "        model(image)\n",
        "    return\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    cnt = 0\n",
        "    for image, target in tqdm(data_loader):\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    model_size = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return model_size\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "def getMiniTestDataset():\n",
        "    # Create a test_loader with batch size = 1\n",
        "    _, test_loader, _ = prepare_data(batch_size=1)\n",
        "\n",
        "    # Prepare to collect 10 images per class\n",
        "    class_images = [[] for _ in range(100)]\n",
        "\n",
        "    # Iterate through the data\n",
        "    for (image, label) in test_loader:\n",
        "        if len(class_images[label]) < 5:\n",
        "            class_images[label].append((image, label))\n",
        "        if all(len(images) == 5 for images in class_images):\n",
        "            break  # Stop once we have 10 images per class\n",
        "\n",
        "    # flatten class_images\n",
        "    mini_test_dataset = []\n",
        "    for images in class_images:\n",
        "        mini_test_dataset.extend(images)\n",
        "    return mini_test_dataset\n",
        "\n",
        "# TA Uses the following code to evaluate your score\n",
        "def lab4_cifar100_evaluation(quantized_model_path='deits_quantized.pth'):\n",
        "    # Prepare data\n",
        "    mini_test_dataset = getMiniTestDataset()\n",
        "\n",
        "    # Load quantized model\n",
        "    quantized_ep = torch.export.load(quantized_model_path)\n",
        "    quantized_model = quantized_ep.module()\n",
        "\n",
        "    # Evaluate model\n",
        "    start_time = time.time()\n",
        "    acc = evaluate_model(quantized_model, mini_test_dataset, device=\"cpu\")\n",
        "    exec_time = time.time() - start_time\n",
        "    model_size = get_size_of_model(quantized_model)\n",
        "\n",
        "    print(f\"Model Size: {model_size:.2f} MB\")\n",
        "    print(f\"Accuracy: {acc:.2f}%\")\n",
        "    print(f\"Execution Time: {exec_time:.2f} s\")\n",
        "\n",
        "    score = 0\n",
        "    if model_size <= 30: score += 10\n",
        "    if model_size <= 27: score += 2 * math.floor(27-model_size)\n",
        "    if acc >= 86:\n",
        "      score += 10 + 2 * math.floor(acc-86)\n",
        "    print(f'Model Score: {score:.2f}')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkZ8DKOfIoXd"
      },
      "source": [
        "## Part1: Simple Quantization Pipeline (0%)\n",
        "\n",
        "Below is a naive pipeline of quantizing DeiT-S. You may need to modify the pipeline or build your own later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbo8e-DgIoXd"
      },
      "source": [
        "[**use_reference_representation=False** in **convert_pt2e()** represents fake quant (matmul using fp32).](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq_static.html#convert-the-calibrated-model-to-a-quantized-model)\n",
        "\n",
        "However when the variable is set to True, the execution speed becomes extremely slow.\n",
        "\n",
        "In this lab, it is just fine to set **use_reference_representation=False**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace functions XNNPackQuantizer uses for annotation.\n",
        "\n",
        "def get_module_names(name):\n",
        "    names = name.split(\".\")\n",
        "    return [\".\".join(names[i:]) for i in reversed(range(len(names)))]\n",
        "\n",
        "def parse_string(name):\n",
        "    if name.startswith(\"L\"): return name[10:]\n",
        "    split_getattr = name.split(\")\")\n",
        "    ig_left = split_getattr[0].split(\"L['self'].\")[-1].split(\",\")[0]\n",
        "    ig_right = split_getattr[0].split(\", '\")[-1][:-1]\n",
        "    return ig_left + \".\" + str(ig_right) + split_getattr[1]\n",
        "\n",
        "def is_name_in_ignore_list(name, IGNORE_LIST):\n",
        "    return name in IGNORE_LIST\n",
        "\n",
        "def name_not_in_ignore_list(n, IGNORE_LIST) -> bool:\n",
        "    nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "    names = [n for n, klass in nn_module_stack.values()]\n",
        "    if len(names) == 0:\n",
        "        return True\n",
        "\n",
        "    names = get_module_names(parse_string(names[-1]))\n",
        "    set1 = set(names)\n",
        "    set2 = set(IGNORE_LIST)\n",
        "    # if len(set1.intersection(set2)) == 0:\n",
        "    #     print(\"DEBUG: \", names)\n",
        "    return len(set1.intersection(set2)) == 0\n",
        "\n",
        "def get_module_name_filter(module_name: str, IGNORE_LIST):\n",
        "    def module_name_filter(n) -> bool:\n",
        "        nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "        names = [n for n, klass in nn_module_stack.values()]\n",
        "        if len(names) == 0:\n",
        "            return False\n",
        "\n",
        "        names = get_module_names(parse_string(names[-1]))\n",
        "        return (module_name in names) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "    return module_name_filter\n",
        "\n",
        "\n",
        "def get_module_type_filter(tp, IGNORE_LIST):\n",
        "    def module_type_filter(n) -> bool:\n",
        "        nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "        types = [t for _, t in nn_module_stack.values()]\n",
        "        return (tp in types) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "\n",
        "    return module_type_filter\n",
        "\n",
        "\n",
        "def get_not_module_type_or_name_filter(\n",
        "    tp_list, module_name_list, IGNORE_LIST\n",
        "):\n",
        "    module_type_filters = [get_module_type_filter(tp) for tp in tp_list]\n",
        "    module_name_list_filters = [get_module_name_filter(m) for m in module_name_list]\n",
        "\n",
        "    def not_module_type_or_name_filter(n) -> bool:\n",
        "        return not any(f(n) for f in module_type_filters + module_name_list_filters) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "\n",
        "    return not_module_type_or_name_filter\n",
        "\n",
        "class PartialXNNPACKQuantizer(XNNPACKQuantizer): # skips quantizing layers inside the ignore_list\n",
        "    def __init__(self, ignore_list):\n",
        "        super().__init__()\n",
        "        self.ignore_list = ignore_list\n",
        "\n",
        "    def _annotate_for_static_quantization_config(\n",
        "        self, model: torch.fx.GraphModule\n",
        "    ) -> torch.fx.GraphModule:\n",
        "        print(\"annotating for static quantization\")\n",
        "        module_name_list = list(self.module_name_config.keys())\n",
        "        for module_name, config in self.module_name_config.items():\n",
        "            self._annotate_all_static_patterns(\n",
        "                model, config, get_module_name_filter(module_name, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        tp_list = list(self.module_type_config.keys())\n",
        "        for module_type, config in self.module_type_config.items():\n",
        "            self._annotate_all_static_patterns(\n",
        "                model, config, get_module_type_filter(module_type, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        self._annotate_all_static_patterns(\n",
        "            model,\n",
        "            self.global_config,\n",
        "            get_not_module_type_or_name_filter(tp_list, module_name_list, self.ignore_list),\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def _annotate_for_dynamic_quantization_config(\n",
        "        self, model: torch.fx.GraphModule\n",
        "    ) -> torch.fx.GraphModule:\n",
        "        print(\"annotating for dynamic quantization\")\n",
        "        module_name_list = list(self.module_name_config.keys())\n",
        "        for module_name, config in self.module_name_config.items():\n",
        "            self._annotate_all_dynamic_patterns(\n",
        "                model, config, get_module_name_filter(module_name, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        tp_list = list(self.module_type_config.keys())\n",
        "        for module_type, config in self.module_type_config.items():\n",
        "            self._annotate_all_dynamic_patterns(\n",
        "                model, config, get_module_type_filter(module_type, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        self._annotate_all_dynamic_patterns(\n",
        "            model,\n",
        "            self.global_config,\n",
        "            get_not_module_type_or_name_filter(tp_list, module_name_list, self.ignore_list),\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# quantizer = XNNPACKQuantizer()\n",
        "# quantizer = PartialXNNPACKQuantizer(ignore_list=[\"head\"]) # replace XNNPACKQuantizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9CfbAo1PIoXe"
      },
      "outputs": [],
      "source": [
        "from torch.quantization import get_default_qconfig, prepare, convert\n",
        "\n",
        "def partial_quantize_ptq_model(quantizer, model: nn.Module, device, data_loader, per_channel=False) -> None:\n",
        "    _dummy_input_data = (next(iter(data_loader))[0].to(device),)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    model = capture_pre_autograd_graph(model, _dummy_input_data)\n",
        "\n",
        "    quantization_config = get_symmetric_quantization_config(is_per_channel=per_channel, is_qat=False)\n",
        "    quantizer.set_global(quantization_config)\n",
        "    # prepare_pt2e folds BatchNorm operators into preceding Conv2d operators, and inserts observers in appropriate places in the model.\n",
        "    model = prepare_pt2e(model, quantizer)\n",
        "\n",
        "    # model(*_dummy_input_data)\n",
        "    #get 128 input data for calibration\n",
        "    for i, (image, _) in enumerate(data_loader):\n",
        "        if i >= 128:\n",
        "            break\n",
        "        image = image.to(device)\n",
        "        model(image)\n",
        "\n",
        "    model = convert_pt2e(model, use_reference_representation=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_qat_model(model: nn.Module, quantizer, device) -> None:\n",
        "    ############### YOUR CODE STARTS HERE ###############\n",
        "\n",
        "    # Step 1. program capture\n",
        "    example_inputs = (torch.randn(1, 3, 224, 224).to(device),)\n",
        "    model = capture_pre_autograd_graph(model, example_inputs)\n",
        "    # Step 2. set quantizatizer\n",
        "    # quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\n",
        "    # Step 3. prepare qat pt2e\n",
        "    model = prepare_qat_pt2e(model, quantizer)\n",
        "\n",
        "    ############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ignore_list = [\"blocks.9.mlp.fc1\", \"blocks.2.attn.qkv\", \"blocks.10.mlp.fc1\"]\n",
        "ignore_list = [\"cls.token\", \"blocks.2.attn.qkv\"]\n",
        "quantizer = PartialXNNPACKQuantizer(ignore_list=ignore_list) # replace XNNPACKQuantizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I_13WgBMIoXe"
      },
      "outputs": [],
      "source": [
        "# batch_size = 1\n",
        "# model = torch.load('0.9099_deit3_small_patch16_224.pth', map_location='cpu')\n",
        "# model = model.to(device)\n",
        "# train_loader, test_loader, nb_classes = prepare_data(batch_size)\n",
        "\n",
        "# simple_test_loader = data_loader_to_list(test_loader)\n",
        "\n",
        "# mini_test_dataset = getMiniTestDataset()\n",
        "# # evaluate before quantization\n",
        "# print('Before quantization:')\n",
        "# print('Device:', device)\n",
        "# acc = evaluate_model(model, mini_test_dataset, device)\n",
        "# # acc = evaluate_model(model, test_loader, device) # acc: 90.99%\n",
        "# print('Size (MB) before quantization:', get_size_of_model(model))\n",
        "# print(f'Accuracy of the model on the test images: {acc}%') # 92.8%\n",
        "# # evaluate_model(model, test_loader, device) # acc: 90.99%\n",
        "\n",
        "# # quantize model\n",
        "# print('Quantizing model...')\n",
        "# model.cpu()\n",
        "# quantized_model = partial_quantize_ptq_model(quantizer, model, device, train_loader, per_channel=False)\n",
        "# torch.ao.quantization.move_exported_model_to_eval(quantized_model)\n",
        "\n",
        "# print('Exporting model...')\n",
        "# quantized_model_path = \"deits_quantized_1.pth\"\n",
        "\n",
        "# quantized_model.cpu()\n",
        "# cpu_example_inputs = (torch.randn([1, 3, 224, 224]), ) # batch_size should equal to 1 on inference.\n",
        "# quantized_ep = torch.export.export(quantized_model, cpu_example_inputs)\n",
        "# torch.export.save(quantized_ep, quantized_model_path)\n",
        "\n",
        "# print('Evaluating model...')\n",
        "# lab4_cifar100_evaluation(quantized_model_path) # 84.4%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "ename": "TorchRuntimeError",
          "evalue": "Failed running call_function <built-in function add>(*(FakeTensor(..., size=(1, 196, 384), grad_fn=<TransposeBackward0>), Parameter(FakeTensor(..., device='cuda:0', size=(1, 196, 384), requires_grad=True))), **{}):\nUnhandled FakeTensor Device Propagation for aten.add.Tensor, found two different devices cpu, cuda:0\n\nfrom user code:\n   File \"/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/timm/models/vision_transformer.py\", line 704, in forward\n    x = self.forward_features(x)\n  File \"/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/timm/models/vision_transformer.py\", line 682, in forward_features\n    x = self._pos_embed(x)\n  File \"/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/timm/models/vision_transformer.py\", line 619, in _pos_embed\n    x = x + pos_embed\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m num_epochs_between_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 17\u001b[0m prepared_model  \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_qat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m prepared_model \u001b[38;5;241m=\u001b[39m prepared_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# QAT takes time and one needs to train over a few epochs.\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mprepare_qat_model\u001b[0;34m(model, quantizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_qat_model\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, quantizer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m############### YOUR CODE STARTS HERE ###############\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Step 1. program capture\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),)\n\u001b[0;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcapture_pre_autograd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Step 2. set quantizatizer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Step 3. prepare qat pt2e\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m prepare_qat_pt2e(model, quantizer)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_export/__init__.py:151\u001b[0m, in \u001b[0;36mcapture_pre_autograd_graph\u001b[0;34m(f, args, kwargs, dynamic_shapes)\u001b[0m\n\u001b[1;32m    145\u001b[0m decomp_table \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    146\u001b[0m     op: op\u001b[38;5;241m.\u001b[39mdecompose\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m FunctionalTensor\u001b[38;5;241m.\u001b[39mmaybe_aliasing_or_mutating_ops\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m    149\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpatch(dataclasses\u001b[38;5;241m.\u001b[39masdict(DEFAULT_EXPORT_DYNAMO_CONFIG)):\n\u001b[0;32m--> 151\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_static_by_default\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msymbolic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecomposition_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecomp_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43maten_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_log_export_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    165\u001b[0m     _, _, _, fake_mode \u001b[38;5;241m=\u001b[39m _convert_input_to_fake(m, args, kwargs)\n\u001b[1;32m    167\u001b[0m     m\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_constraints\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    168\u001b[0m         k: v\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fake_mode\u001b[38;5;241m.\u001b[39mshape_env\u001b[38;5;241m.\u001b[39mvar_to_range\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^[if]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(k))\n\u001b[1;32m    171\u001b[0m     }\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1311\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;66;03m# TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1311\u001b[0m     result_traced \u001b[38;5;241m=\u001b[39m \u001b[43mopt_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1313\u001b[0m     constraint_violation_error \u001b[38;5;241m=\u001b[39m e\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    386\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    388\u001b[0m signpost_event(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     },\n\u001b[1;32m    398\u001b[0m )\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 676\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    679\u001b[0m     Unsupported,\n\u001b[1;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m     BisectValidationException,\n\u001b[1;32m    688\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    533\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1033\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1036\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 500\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    502\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2149\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:489\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    488\u001b[0m     )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1219\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1217\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1218\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:674\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:335\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    328\u001b[0m         module_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m module_attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant\n\u001b[1;32m    331\u001b[0m     ):\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcall_method(\n\u001b[1;32m    333\u001b[0m             tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, args, kwargs, constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant\n\u001b[1;32m    334\u001b[0m         )\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:289\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    286\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    287\u001b[0m     )\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:680\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2285\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2399\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2399\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mSkipFrame \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2401\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSKIPPED INLINING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:489\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    488\u001b[0m     )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1219\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1217\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopn(inst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   1218\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1219\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:674\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:335\u001b[0m, in \u001b[0;36mUserMethodVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    328\u001b[0m         module_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m module_attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant\n\u001b[1;32m    331\u001b[0m     ):\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcall_method(\n\u001b[1;32m    333\u001b[0m             tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, args, kwargs, constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant\n\u001b[1;32m    334\u001b[0m         )\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:289\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[1;32m    286\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[1;32m    287\u001b[0m     )\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:90\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_function\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m, tx, args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:680\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2285\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[0;32m-> 2285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2399\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[0;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[0;32m-> 2399\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mSkipFrame \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2401\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSKIPPED INLINING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:242\u001b[0m, in \u001b[0;36mstack_op.<locals>.impl\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpl\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslatorBase\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst: Instruction):\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn_var\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/builtin.py:641\u001b[0m, in \u001b[0;36mBuiltinVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mtruediv \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    638\u001b[0m             args[\u001b[38;5;241m0\u001b[39m], variables\u001b[38;5;241m.\u001b[39mUnspecializedPythonVariable\n\u001b[1;32m    639\u001b[0m         ):\n\u001b[1;32m    640\u001b[0m             args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconvert_to_constant(tx)\n\u001b[0;32m--> 641\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial tensor op: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1330\u001b[0m, in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   1322\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx\u001b[39m\u001b[38;5;124m\"\u001b[39m: tx,\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n\u001b[1;32m   1328\u001b[0m }\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subclass_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorVariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrap_fx_proxy_cls(target_cls\u001b[38;5;241m=\u001b[39mTensorWithTFOverrideVariable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py:1415\u001b[0m, in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m preserve_rng_state():\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;66;03m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m         \u001b[38;5;66;03m# cases properly below.\u001b[39;00m\n\u001b[0;32m-> 1415\u001b[0m         example_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_fake_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_graph_fake\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# Handle recursive calls here\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m maybe_get_fake_mode(example_value) \u001b[38;5;129;01mis\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode:\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:1714\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cause, ValueRangeError):\n\u001b[1;32m   1713\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UserError(UserErrorType\u001b[38;5;241m.\u001b[39mCONSTRAINT_VIOLATION, e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchRuntimeError(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_non_graph_fake:\n\u001b[1;32m   1717\u001b[0m     _ \u001b[38;5;241m=\u001b[39m tree_map_only(\n\u001b[1;32m   1718\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, functools\u001b[38;5;241m.\u001b[39mpartial(ensure_graph_fake, tx\u001b[38;5;241m=\u001b[39mtx), ret_val\n\u001b[1;32m   1719\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:1656\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[0;32m-> 1656\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_fake_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:1190\u001b[0m, in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_fake_exception\u001b[39m(fn):\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1192\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unimplemented\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:1657\u001b[0m, in \u001b[0;36mget_fake_value.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[1;32m   1656\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m wrap_fake_exception(\n\u001b[0;32m-> 1657\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m         )\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:1782\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m unimplemented(make_error_message(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1782\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(make_error_message(e))\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1783\u001b[0m             e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1784\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(op)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_dynamo/utils.py:1764\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1764\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1765\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1766\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], node\u001b[38;5;241m.\u001b[39mtarget)(\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/utils/_stats.py:20\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:896\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    893\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    894\u001b[0m ), func\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1241\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:966\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crosscheck_cache_output(output, func, types, args, kwargs)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cache_entry(key, func, args, kwargs, output)\n\u001b[1;32m    968\u001b[0m     FakeTensorMode\u001b[38;5;241m.\u001b[39mcache[key] \u001b[38;5;241m=\u001b[39m entry\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1462\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m not_implemented_error:\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_run_unsafe_fallback(not_implemented_error)\n\u001b[0;32m-> 1462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_meta_outputs_with_default_device_logic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1592\u001b[0m, in \u001b[0;36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic\u001b[0;34m(self, r, func, flat_args, device)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[0;32m-> 1592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/utils/_pytree.py:900\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m    898\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m    899\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/utils/_pytree.py:736\u001b[0m, in \u001b[0;36mTreeSpec.unflatten\u001b[0;34m(self, leaves)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munflatten\u001b[39m(\u001b[38;5;28mself\u001b[39m, leaves: Iterable[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTree:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(leaves, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 736\u001b[0m         leaves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleaves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(leaves) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_leaves:\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    739\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreespec.unflatten(leaves): `leaves` has length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(leaves)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the spec refers to a pytree that holds \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_leaves\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    741\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    742\u001b[0m         )\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:1567\u001b[0m, in \u001b[0;36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic.<locals>.wrap\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m has_scalar_only_inputs\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m common_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1564\u001b[0m     (\n\u001b[1;32m   1565\u001b[0m         common_device,\n\u001b[1;32m   1566\u001b[0m         has_scalar_only_inputs,\n\u001b[0;32m-> 1567\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mFakeTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_common_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_our_fake(e):\n\u001b[1;32m   1570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m   1571\u001b[0m         e\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m common_device,\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFakeTensor is wrapped to wrong device, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1573\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:638\u001b[0m, in \u001b[0;36mFakeTensor._find_common_device\u001b[0;34m(func, flat_args)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found two different devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m flat_args:\n\u001b[0;32m--> 638\u001b[0m     \u001b[43mmerge_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# some functions that allow Python numbers to bind to Tensors\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# if we have failed to find a device, and we're running one of these operators,\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# we must have scalar only inputs\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_allow_numbers_as_tensors(func) \u001b[38;5;129;01mand\u001b[39;00m common_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# ops with scalar only inputs always have result on cpu\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lab4/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py:633\u001b[0m, in \u001b[0;36mFakeTensor._find_common_device.<locals>.merge_devices\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# mismatching devices of non-zero dim tensors, throw\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# This might be valid behavior and need to be explicitly modeled, e.g. reshape_as\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found two different devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m )\n",
            "\u001b[0;31mTorchRuntimeError\u001b[0m: Failed running call_function <built-in function add>(*(FakeTensor(..., size=(1, 196, 384), grad_fn=<TransposeBackward0>), Parameter(FakeTensor(..., device='cuda:0', size=(1, 196, 384), requires_grad=True))), **{}):\nUnhandled FakeTensor Device Propagation for aten.add.Tensor, found two different devices cpu, cuda:0\n\nfrom user code:\n   File \"/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/timm/models/vision_transformer.py\", line 704, in forward\n    x = self.forward_features(x)\n  File \"/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/timm/models/vision_transformer.py\", line 682, in forward_features\n    x = self._pos_embed(x)\n  File \"/home/aa35037123/miniconda3/envs/lab4/lib/python3.10/site-packages/timm/models/vision_transformer.py\", line 619, in _pos_embed\n    x = x + pos_embed\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 1\n",
        "model = torch.load('0.9099_deit3_small_patch16_224.pth', map_location='cpu')\n",
        "model = model.to(device)\n",
        "train_loader, test_loader, nb_classes = prepare_data(batch_size)\n",
        "\n",
        "############### YOUR CODE STARTS HERE ###############\n",
        "num_epochs = 3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "############### YOUR CODE ENDS HERE #################\n",
        "\n",
        "num_observer_update_epochs = 4\n",
        "num_batch_norm_update_epochs = 3\n",
        "num_epochs_between_evals = 2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "prepared_model  = prepare_qat_model(model, quantizer, device)\n",
        "prepared_model = prepared_model.to(device)\n",
        "\n",
        "# QAT takes time and one needs to train over a few epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(prepared_model, criterion, optimizer, train_loader, device)\n",
        "\n",
        "    # Optionally disable observer/batchnorm stats after certain number of epochs\n",
        "    if epoch >= num_observer_update_epochs:\n",
        "        print(\"Disabling observer for subseq epochs, epoch = \", epoch)\n",
        "        prepared_model.apply(torch.ao.quantization.disable_observer)\n",
        "    if epoch >= num_batch_norm_update_epochs:\n",
        "        print(\"Freezing BN for subseq epochs, epoch = \", epoch)\n",
        "        for n in prepared_model.graph.nodes:\n",
        "            # Args: input, weight, bias, running_mean, running_var, training, momentum, eps\n",
        "            # set the `training` flag to False here to freeze BN stats\n",
        "            if n.target in [\n",
        "                torch.ops.aten._native_batch_norm_legit.default,\n",
        "                torch.ops.aten.cudnn_batch_norm.default,\n",
        "            ]:\n",
        "                new_args = list(n.args)\n",
        "                new_args[5] = False\n",
        "                n.args = new_args\n",
        "        prepared_model.recompile()\n",
        "\n",
        "    # Check the quantized accuracy every N epochs\n",
        "\n",
        "    if (epoch + 1) % num_epochs_between_evals == 0:\n",
        "        prepared_model_copy = copy.deepcopy(prepared_model)\n",
        "        quantized_model = convert_pt2e(prepared_model_copy)\n",
        "        acc = evaluate_model(quantized_model, test_loader,device)\n",
        "\n",
        "print('Exporting model...')\n",
        "quantized_model_path = \"deits_quantized_1.pth\"\n",
        "\n",
        "quantized_model.cpu()\n",
        "cpu_example_inputs = (torch.randn([1, 3, 224, 224]), ) # batch_size should equal to 1 on inference.\n",
        "quantized_ep = torch.export.export(quantized_model, cpu_example_inputs)\n",
        "torch.export.save(quantized_ep, quantized_model_path)\n",
        "\n",
        "print('Evaluating model...')\n",
        "lab4_cifar100_evaluation(quantized_model_path) # 84.4%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Ou1NYqlNIoXf",
        "V_xk1sSXoqsJ",
        "NO1-XuLJBkL2",
        "bztBdOxJRKTD"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
